{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________________________________________________\n",
      "Loading Data...........\n"
     ]
    }
   ],
   "source": [
    "## Loading Dataset\n",
    "print(\"_______________________________________________________________\")\n",
    "print(\"Loading Data...........\")\n",
    "cols = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"]\n",
    "data = pd.read_csv(\n",
    "    r\"test.csv\",\n",
    "    header=None,\n",
    "    names=cols,\n",
    "    engine=\"python\",\n",
    "    encoding=\"latin1\"\n",
    ")\n",
    "data.drop([\"id\", \"date\", \"query\", \"user\"],\n",
    "          axis=1,\n",
    "          inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________________________________________________\n",
      "Data Pre-processing...........\n"
     ]
    }
   ],
   "source": [
    "print(\"_______________________________________________________________\")\n",
    "print(\"Data Pre-processing...........\")\n",
    "def clean_tweet(tweet):\n",
    "    tweet = BeautifulSoup(tweet, \"lxml\").get_text()\n",
    "    # Removing the @\n",
    "    tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet)\n",
    "    # Removing the URL links\n",
    "    tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', tweet)\n",
    "    # Keeping only letters\n",
    "    tweet = re.sub(r\"[^a-zA-Z.!?']\", ' ', tweet)\n",
    "    # Removing additional whitespaces\n",
    "    tweet = re.sub(r\" +\", ' ', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = [clean_tweet(tweet) for tweet in data.text]\n",
    "labels = data.sentiment.values\n",
    "labels[labels == 4] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "output_dir = r'C:\\Users\\tapojyoti.paul\\Documents\\Intel\\BERT\\Pytorch Model Bert'\n",
    "# Load a trained model and vocabulary that you have fine-tuned\n",
    "model = BertForSequenceClassification.from_pretrained(output_dir)\n",
    "tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "# Copy the model to the GPU.\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_clean\n",
    "def get_test_data(size: int = 1):\n",
    "    \"\"\"Generates a test dataset of the specified size\"\"\" \n",
    "    num_rows = len(X)\n",
    "    test_df = X.copy()\n",
    "\n",
    "    while num_rows < size:\n",
    "        test_df = test_df + test_df\n",
    "        num_rows = len(test_df)\n",
    "\n",
    "    return test_df[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________________________________________________\n",
      "Inferencing Started...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1229 16:05:56.932788 16196 <ipython-input-10-84962f535538>:124] #, median, mean, std_dev, min_time, max_time, quantile_10, quantile_90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data prep and inference for 1 sentence(s)..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress..: 100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 501.47it/s]\n",
      "Progress..: 100%|████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Progress..: 100%|████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Progress..: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 91.26it/s]\n",
      "Progress..: 100%|████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Progress..: 100%|████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Progress..: 100%|████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Progress..: 100%|████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Progress..: 100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 501.05it/s]\n",
      "Progress..: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 75.42it/s]\n",
      "Progress..: 100%|████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Progress..: 100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 334.34it/s]\n",
      "Progress..: 100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 500.69it/s]\n",
      "Progress..: 100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 494.55it/s]\n",
      "Progress..: 100%|████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Progress..: 100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 107.41it/s]\n",
      "Progress..: 100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 123.48it/s]\n",
      "Progress..: 100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 501.17it/s]\n",
      "Progress..: 100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 935.60it/s]\n",
      "Progress..: 100%|████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Progress..: 100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 334.21it/s]\n",
      "Progress..: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 63.97it/s]\n",
      "Progress..: 100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 334.37it/s]\n",
      "Progress..: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 98.48it/s]\n",
      "Progress..: 100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 123.74it/s]\n",
      "Progress..: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 64.02it/s]\n",
      "Progress..: 100%|████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Progress..: 100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 113.69it/s]\n",
      "Progress..: 100%|████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Progress..: 100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 333.94it/s]\n",
      "Progress..: 100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 105.24it/s]\n",
      "Progress..: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 99.25it/s]\n",
      "Progress..: 100%|████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Progress..: 100%|████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Progress..: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 97.69it/s]\n",
      "Progress..: 100%|████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Progress..: 100%|████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Progress..: 100%|████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Progress..: 100%|████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Progress..: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 99.24it/s]\n",
      "Progress..: 100%|████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Progress..: 100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 124.63it/s]\n",
      "Progress..: 100%|████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Progress..: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 64.04it/s]\n",
      "Progress..: 100%|████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Progress..: 100%|████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Progress..: 100%|████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Progress..: 100%|████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Progress..: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 98.86it/s]\n",
      "Progress..: 100%|████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of predicted df 1\n",
      "1 ,        median        mean        std_dev   min_time   max_time  quantile_10  \\\n",
      "0   100158.0   107537.86   29012.722779    73773.0   233599.0      83734.9   \n",
      "0     6356.5     6490.48    2665.935560     1627.0    15007.0       2749.8   \n",
      "0   107232.5   114028.34   29113.247762    84157.0   236266.0      88599.5   \n",
      "0  2041719.5  2170719.98  416322.361473  1511379.0  3410131.0    1781055.1   \n",
      "0  2151914.0  2284748.32  428045.406190  1597367.0  3505439.0    1892838.2   \n",
      "\n",
      "   quantile_90                   Flag  \n",
      "0     136129.0              Only Bert  \n",
      "0      10084.2          Prep w/o Bert  \n",
      "0     140867.5         Prep with Bert  \n",
      "0    2719548.4         Inference Time  \n",
      "0    2938582.8  Prep & Inf Time Total  \n",
      "running data prep and inference for 10 sentence(s)..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress..: 100%|████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1114.08it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 983.22it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 527.70it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 551.37it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 820.32it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 495.36it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 798.28it/s]\n",
      "Progress..: 100%|████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1009.56it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 492.04it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 497.57it/s]\n",
      "Progress..: 100%|████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1004.98it/s]\n",
      "Progress..: 100%|████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1137.59it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 550.59it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 829.67it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 835.29it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 449.43it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 808.98it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 670.82it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 826.28it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 491.30it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 925.43it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 533.22it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 764.71it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 494.68it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 613.57it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 972.39it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 853.99it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 525.44it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 487.79it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 662.41it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 424.32it/s]\n",
      "Progress..: 100%|████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1071.42it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 534.20it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 836.29it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 813.94it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 499.40it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 919.12it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 421.93it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 496.62it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 493.29it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 787.16it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 549.92it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 457.67it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 835.57it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 429.66it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 716.25it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 835.57it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 450.64it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 770.74it/s]\n",
      "Progress..: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 666.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of predicted df 10\n",
      "10 ,         median         mean        std_dev   min_time   max_time  quantile_10  \\\n",
      "0    20809.50    23462.314   14732.811130    14530.9   122598.4     16232.73   \n",
      "0      869.40     1015.836    1035.982739      317.7     7702.7       558.22   \n",
      "0    21701.90    24478.150   15680.559572    15408.7   130301.1     17200.68   \n",
      "0  1491818.20  1535361.300  189451.418500  1283627.8  2269201.7   1392124.93   \n",
      "0  1514233.05  1559839.450  198718.635588  1301332.1  2399502.8   1412945.16   \n",
      "\n",
      "   quantile_90                   Flag  \n",
      "0     26774.61              Only Bert  \n",
      "0      1260.33          Prep w/o Bert  \n",
      "0     28117.88         Prep with Bert  \n",
      "0   1678581.61         Inference Time  \n",
      "0   1700216.76  Prep & Inf Time Total  \n",
      "Summary........\n",
      "       median         mean        std_dev   min_time   max_time  quantile_10  \\\n",
      "0   100158.00   107537.860   29012.722779    73773.0   233599.0     83734.90   \n",
      "0     6356.50     6490.480    2665.935560     1627.0    15007.0      2749.80   \n",
      "0   107232.50   114028.340   29113.247762    84157.0   236266.0     88599.50   \n",
      "0  2041719.50  2170719.980  416322.361473  1511379.0  3410131.0   1781055.10   \n",
      "0  2151914.00  2284748.320  428045.406190  1597367.0  3505439.0   1892838.20   \n",
      "0    20809.50    23462.314   14732.811130    14530.9   122598.4     16232.73   \n",
      "0      869.40     1015.836    1035.982739      317.7     7702.7       558.22   \n",
      "0    21701.90    24478.150   15680.559572    15408.7   130301.1     17200.68   \n",
      "0  1491818.20  1535361.300  189451.418500  1283627.8  2269201.7   1392124.93   \n",
      "0  1514233.05  1559839.450  198718.635588  1301332.1  2399502.8   1412945.16   \n",
      "\n",
      "   quantile_90                   Flag  No_of_Observation  \n",
      "0    136129.00              Only Bert                  1  \n",
      "0     10084.20          Prep w/o Bert                  1  \n",
      "0    140867.50         Prep with Bert                  1  \n",
      "0   2719548.40         Inference Time                  1  \n",
      "0   2938582.80  Prep & Inf Time Total                  1  \n",
      "0     26774.61              Only Bert                 10  \n",
      "0      1260.33          Prep w/o Bert                 10  \n",
      "0     28117.88         Prep with Bert                 10  \n",
      "0   1678581.61         Inference Time                 10  \n",
      "0   1700216.76  Prep & Inf Time Total                 10  \n"
     ]
    }
   ],
   "source": [
    "def calculate_stats(time_list):\n",
    "    \"\"\"Calculate mean and standard deviation of a list\"\"\"\n",
    "    time_array = np.array(time_list)\n",
    "\n",
    "    median = np.median(time_array)\n",
    "    mean = np.mean(time_array)\n",
    "    std_dev = np.std(time_array)\n",
    "    max_time = np.amax(time_array)\n",
    "    min_time = np.amin(time_array)\n",
    "    quantile_10 = np.quantile(time_array, 0.1)\n",
    "    quantile_90 = np.quantile(time_array, 0.9)\n",
    "\n",
    "    basic_key = [\"median\",\"mean\",\"std_dev\",\"min_time\",\"max_time\",\"quantile_10\",\"quantile_90\"]\n",
    "    basic_value = [median,mean,std_dev,min_time,max_time,quantile_10,quantile_90]\n",
    "\n",
    "    dict_basic = dict(zip(basic_key, basic_value))\n",
    "    \n",
    "    return pd.DataFrame(dict_basic, index = [0])\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "from pathlib import Path\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "NUM_LOOPS = 50\n",
    "\n",
    "def run_inference(num_observations:int = 1000):\n",
    "    \"\"\"Run xgboost for specified number of observations\"\"\"\n",
    "    # Load data\n",
    "    test_twt = get_test_data(num_observations)\n",
    "    num_rows = len(test_twt)\n",
    "    print(f\"running data prep and inference for {num_rows} sentence(s)..\")\n",
    "    \n",
    "    run_times = []\n",
    "    bert_times = []\n",
    "    prep_time_wo_berts = []\n",
    "    prep_time_alls = []\n",
    "    prep_inf_times = []\n",
    "    inference_times = []\n",
    "    \n",
    "    for _ in range(NUM_LOOPS):\n",
    "        \n",
    "#######################################################################################################################\n",
    "        st_tm_bert = timer()\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "        # For every sentence...\n",
    "        for sent in tqdm.tqdm(test_twt,desc =\"Progress..\"):\n",
    "            encoded_dict = tokenizer.encode_plus(\n",
    "                                sent,                      # Sentence to encode.\n",
    "                                add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                                max_length = 64,           # Pad & truncate all sentences.\n",
    "                                pad_to_max_length = True,\n",
    "                                return_attention_mask = True,   # Construct attn. masks.\n",
    "                                return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                           )\n",
    "            # Add the encoded sentence to the list.    \n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "            # And its attention mask (simply differentiates padding from non-padding).\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "        # Convert the lists into tensors.\n",
    "        input_ids = torch.cat(input_ids, dim=0)\n",
    "        attention_masks = torch.cat(attention_masks, dim=0)\n",
    "        batch_size = 32  \n",
    "        # Create the DataLoader.\n",
    "        prediction_data = TensorDataset(input_ids, attention_masks,)\n",
    "        prediction_dataloader = DataLoader(prediction_data,batch_size=num_rows)\n",
    "        \n",
    "        end_tm_bert = timer()\n",
    "        \n",
    "        for batch in prediction_dataloader:\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "#             print(\"/////////////\")\n",
    "#             print(len(b_input_ids))\n",
    "#             print(\"/////////////\")\n",
    "#######################################################################################################################\n",
    "        start_time = timer()\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        logits = outputs[0]\n",
    "        end_time = timer()\n",
    "#######################################################################################################################\n",
    "\n",
    "        total_time = end_time - start_time\n",
    "        run_times.append(total_time*10e3)\n",
    "        \n",
    "        bert_time = (end_tm_bert-st_tm_bert)*(10e6)/num_rows\n",
    "        prep_time_wo_bert = (start_time-end_tm_bert)*(10e6)/num_rows\n",
    "        prep_time_all = (start_time-st_tm_bert)*(10e6)/num_rows\n",
    "        inference_time = total_time*(10e6)/num_rows\n",
    "        prep_inf_time = (end_time-st_tm_bert)*(10e6)/num_rows\n",
    "        \n",
    "        bert_times.append(bert_time)\n",
    "        prep_time_wo_berts.append(prep_time_wo_bert)\n",
    "        prep_time_alls.append(prep_time_all)\n",
    "        prep_inf_times.append(prep_inf_time)\n",
    "        inference_times.append(inference_time)\n",
    "        \n",
    "    print(\"length of predicted df\", len(logits))\n",
    "    \n",
    "    df1 = calculate_stats(bert_times)\n",
    "    df1[\"Flag\"] = \"Only Bert\"\n",
    "    df2 = calculate_stats(prep_time_wo_berts)\n",
    "    df2[\"Flag\"] = \"Prep w/o Bert\"\n",
    "    df3 = calculate_stats(prep_time_alls)\n",
    "    df3[\"Flag\"] = \"Prep with Bert\"\n",
    "    df4 = calculate_stats(prep_inf_times)\n",
    "    df4[\"Flag\"] = \"Prep & Inf Time Total\"\n",
    "    df5 = calculate_stats(inference_times)\n",
    "    df5[\"Flag\"] = \"Inference Time\"\n",
    "\n",
    "    dfs = pd.concat([df1,df2,df3,df5,df4])\n",
    "    \n",
    "    print(num_observations, \", \", dfs)\n",
    "    return dfs\n",
    "\n",
    "STATS = '#, median, mean, std_dev, min_time, max_time, quantile_10, quantile_90'\n",
    "\n",
    "print(\"_______________________________________________________________\")\n",
    "print(\"Inferencing Started...........\")\n",
    "if __name__=='__main__':\n",
    "    ob_ct = 1  # Start with a single observation\n",
    "    logging.info(STATS)\n",
    "    temp_df = pd.DataFrame()\n",
    "    while ob_ct <= 10:\n",
    "        temp = run_inference(ob_ct)\n",
    "        temp[\"No_of_Observation\"] = ob_ct\n",
    "        temp_df = temp_df.append(temp)\n",
    "        ob_ct *= 10\n",
    "    print(\"Summary........\")\n",
    "    print(temp_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
